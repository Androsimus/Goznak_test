{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path to folder with test data\n",
    "There must be to directories inside: \"clean\" and \"noisy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'data/val/val/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuda device checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your device is \"cuda:0\"\n"
     ]
    }
   ],
   "source": [
    "device = ''\n",
    "if torch.cuda.is_available():  \n",
    "    device = \"cuda:0\" \n",
    "else:  \n",
    "    device = \"cpu\"\n",
    "print(f'Your device is \"{device}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form a list of directories which contain train or val samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_data_paths(path: str):\n",
    "    \"\"\"\n",
    "    input: path to train or val dir with 'clean' and 'noise' dirs\n",
    "    output: list of paths to clean data\n",
    "    \"\"\"\n",
    "    clean = 'clean/'\n",
    "    list_clean = []\n",
    "    with os.scandir(path+clean) as entries:\n",
    "        for entry in entries:\n",
    "            if entry.is_dir():\n",
    "                list_clean.append(entry.path)\n",
    "    return sorted(list_clean)\n",
    "\n",
    "def check_data_paths(paths_list, show_cnt = 10):\n",
    "    \"\"\"\n",
    "    outputs number of input list elements and their values\n",
    "    \"\"\"\n",
    "    print(f'input list length = {len(paths_list)}')\n",
    "    for i, elem in enumerate(paths_list):\n",
    "        print(elem)\n",
    "        if i==show_cnt:\n",
    "            break\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, transpose and check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_clean_data_paths(clean_data_paths: list):\n",
    "    \"\"\"\n",
    "    Loading numpy arrays situated on input list paths.\n",
    "    input: paths to clean data\n",
    "    output: loaded clean and noisy data to lists of np.ndarrays    \n",
    "    \"\"\"\n",
    "    clean_data_list = []\n",
    "    noisy_data_list = []\n",
    "\n",
    "    #scans every dir path\n",
    "    for path in clean_data_paths:\n",
    "        #scans every file in current dir\n",
    "        with os.scandir(path) as entries:\n",
    "            for entry in entries:\n",
    "                if entry.is_file():\n",
    "                    clean_path = entry.path\n",
    "                    noisy_path = entry.path.replace('clean', 'noisy',1)\n",
    "\n",
    "                    clean_data_list.append(np.load(clean_path).T)\n",
    "                    noisy_data_list.append(np.load(noisy_path).T)\n",
    "    return clean_data_list, noisy_data_list\n",
    "\n",
    "def check_loaded_data(data1_list, data2_list, check_part=0.1, check_shapes=True, print_shapes=False, check_data=False):\n",
    "    \"\"\"\n",
    "    Help checking loaded data: if data from 2 lists has same shape and doesn't contain nans, zeros only and so on\n",
    "    \"\"\"\n",
    "    \n",
    "    cnt2check = int(max(1, len(data1_list)*check_part))\n",
    "    samples2check = np.random.choice(range(len(data1_list)), cnt2check, replace=False)\n",
    "    \n",
    "    different_shape = False\n",
    "    for index in samples2check:\n",
    "        if data1_list[index].shape != data2_list[index].shape:\n",
    "            different_shape = True\n",
    "            print(f'Difference in shape is detected!'\n",
    "                 f'\\t{data1_list[index].shape} vs {data2_list[index].shape}')\n",
    "            break\n",
    "        if print_shapes:\n",
    "            print(data1_list[index].shape)\n",
    "    \n",
    "    if check_data:\n",
    "        for index in samples2check:\n",
    "            print(f'mean1 = {np.mean(data1_list[index]):5.3}, std1 = {np.std(data1_list[index]):5.3}\\t'\n",
    "                  f'mean2 = {np.mean(data2_list[index]):5.3}, std2 = {np.std(data2_list[index]):5.3}')\n",
    "        \n",
    "def get_data_statistics(data):\n",
    "    \"\"\"\n",
    "    Prints some data statistics and plots histogram of data lengths\n",
    "    \"\"\"\n",
    "    dataLens = []\n",
    "    for d in data:\n",
    "        dataLens.append(d.shape[1])\n",
    "    dataLens = np.array(dataLens)\n",
    "    maxLen = np.max(dataLens)\n",
    "    minLen = np.min(dataLens)\n",
    "    meanLen = np.mean(dataLens)\n",
    "    stdLen = np.std(dataLens)\n",
    "    print(f'Data length params: min={minLen}, max={maxLen}, mean={meanLen}, std={stdLen}')\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.histplot(data=dataLens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use PyTorch functionality for batch construction:\n",
    "  - custom Dataset class to store data with different sizes,\n",
    "  - customized collate_fn to form batches of different shape elements in DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionDS(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class.\n",
    "    Maintains\n",
    "        data - clean and noisy data\n",
    "        targets - target values for classification (0 - clean, 1 - noisy)\n",
    "    \"\"\"\n",
    "    def __init__(self, clean, noisy):\n",
    "        #initialize class object\n",
    "        \n",
    "        #concatenate clean and noisy data lists\n",
    "        self.data = clean + noisy\n",
    "        #generate corresponding targets\n",
    "        self.targets = [0 for i in range(len(clean))] + [1 for i in range(len(noisy))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        #standard interface function for Dataset\n",
    "        if self.data != None:\n",
    "            return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        #standard interface function for Dataset\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        sample = [self.data[idx], self.targets[idx]] #{'mel_data': self.data[idx], 'target': self.targets[idx]}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_function(batch):\n",
    "    \"\"\"\n",
    "    A customization of default PyTorch collate_fn function\n",
    "    Forms batch from items of different size:\n",
    "        looks for min item length and then randomly \n",
    "        cuts parts of min length from each item\n",
    "    \"\"\"\n",
    "    #extract data from input batch\n",
    "    data = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    \n",
    "    min_len = 100000\n",
    "    for elem in data:\n",
    "        min_len = min(min_len, elem.shape[1])\n",
    "    \n",
    "    #slicing data to size of minimum element\n",
    "    data = [random_cut(elem, min_len) for elem in data]\n",
    "    \n",
    "    return [torch.tensor(data).float(), torch.tensor(targets).float()]\n",
    "        \n",
    "    \n",
    "def random_cut(array, slice_len):\n",
    "    \"\"\"\n",
    "    Help function to randomly cut array through slicing\n",
    "    \"\"\"\n",
    "    start_cut_range = array.shape[1] - slice_len\n",
    "    random_start = np.random.randint(0, start_cut_range+1)\n",
    "\n",
    "    return array[:, random_start: random_start+slice_len]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DataLoader(clean_data_list, noisy_data_list, batch_size):\n",
    "    \"\"\"\n",
    "    Returns dataloaders for further training and evaluating\n",
    "    Input:\n",
    "        clean_data_list - list of numpy arrays with clean data\n",
    "        noisy_data_list - list of numpy arrays with noisy data\n",
    "        batch_size - defines batch size\n",
    "    \"\"\"\n",
    "    dataset = DetectionDS(clean_data_list, noisy_data_list)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, \n",
    "                            collate_fn=collate_function, pin_memory=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition\n",
    "For noise classification problem we will use 10-convlayer VGG-like CNN with BatchNorm layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryNoiseClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    Model definition.\n",
    "    Layers summary:\n",
    "        There are 5 stages:\n",
    "            1-3 stages consist of 2 Conv1d layers with ReLU -> MaxPool -> BatchNorm.\n",
    "            4th stage consists of 2 Conv1d layers with ReLU -> BatchNorm -> GlobalMaxPool\n",
    "            5th stage consists of 2 Fully-connected layers\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(BinaryNoiseClassification, self).__init__()\n",
    "        #VGG like architecture\n",
    "        self.Conv1_1 = nn.Conv1d(in_channels=80, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.Conv1_2 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.MaxPool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.BatchNorm1 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.Conv2_1 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.Conv2_2 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.MaxPool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.BatchNorm2 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.Conv3_1 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.Conv3_2 = nn.Conv1d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.MaxPool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.BatchNorm3 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.Conv4_1 = nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.Conv4_2 = nn.Conv1d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.BatchNorm4 = nn.BatchNorm1d(512)\n",
    "        self.GloMaxPool1 = nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        \n",
    "        self.Linear1 = nn.Linear(in_features=512, out_features=10)\n",
    "        self.Linear2 = nn.Linear(in_features=10, out_features=1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        #1st stage\n",
    "        x = F.relu(self.Conv1_1(inputs))\n",
    "        x = self.MaxPool1(F.relu(self.Conv1_2(x)))\n",
    "        x = self.BatchNorm1(x)\n",
    "        #2nd stage\n",
    "        x = F.relu(self.Conv2_1(x))\n",
    "        x = self.MaxPool2(F.relu(self.Conv2_2(x)))\n",
    "        x = self.BatchNorm2(x)\n",
    "        #3rd stage\n",
    "        x = F.relu(self.Conv3_1(x))\n",
    "        x = self.MaxPool3(F.relu(self.Conv3_2(x)))\n",
    "        x = self.BatchNorm3(x)\n",
    "        #4th stage\n",
    "        x = F.relu(self.Conv4_1(x))\n",
    "        x = self.GloMaxPool1(F.relu(self.Conv4_2(x)))\n",
    "        x = self.BatchNorm4(x)\n",
    "        #full conv stage\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.Linear1(x))\n",
    "        x = self.Linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions for evaluating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Computes output for data in dataloader and returns loss and accuracy.\n",
    "    Input: \n",
    "        model - trained model, \n",
    "        dataloader - DataLoader with evaluating data,\n",
    "        device - chosen device\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = None\n",
    "    targets = None\n",
    "    for batch_number, data in enumerate(dataloader):\n",
    "        samples, labels = data[0], data[1].unsqueeze_(1)\n",
    "        samples, labels = samples.to(device) , labels.to(device)\n",
    "\n",
    "        if predictions is None:\n",
    "            predictions = model(samples).cpu().detach().numpy()\n",
    "            targets = labels.cpu().detach().numpy()\n",
    "        else:        \n",
    "            predictions = np.concatenate((predictions, model(samples).cpu().detach().numpy()), axis=0)\n",
    "            targets = np.concatenate((targets, labels.cpu().detach().numpy()), axis=0)\n",
    "    \n",
    "    accuracy = binary_acc(predictions, targets)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def binary_acc(y_pred, y_test):\n",
    "    \"\"\"\n",
    "    Calculates objective metric: Accuracy\n",
    "    Input: \n",
    "        y_pred - model predictions, \n",
    "        y_test - ground truth\n",
    "    \"\"\"\n",
    "    y_test, y_pred = torch.from_numpy(y_test).float(), torch.from_numpy(y_pred).float()\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float().item()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = np.round(acc*100, 2)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier evaluation\n",
    "Loading best trained model and compute its loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_classification_model = BinaryNoiseClassification()\n",
    "model_state_dict = torch.load(f'classifier_epoch15.pth')\n",
    "loaded_classification_model.load_state_dict(model_state_dict)\n",
    "loaded_classification_model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean_paths_list = get_clean_data_paths(test_path)\n",
    "test_clean_data, test_noisy_data = get_data_from_clean_data_paths(test_clean_paths_list[:])\n",
    "test_dataloader = get_DataLoader(test_clean_data, test_noisy_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 98.32%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = Evaluate(loaded_classification_model, test_dataloader, device)\n",
    "print(f'Accuracy = {test_accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
