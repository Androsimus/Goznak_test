{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path to folder with test data\n",
    "There must be to directories inside: \"clean\" and \"noisy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'data/val/val/'\n",
    "\n",
    "#you can fill the list below with indeces of data elements to visualize\n",
    "list2visualize = [] #[0,1,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuda device checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your device is \"cuda:0\"\n"
     ]
    }
   ],
   "source": [
    "device = ''\n",
    "if torch.cuda.is_available():  \n",
    "    device = \"cuda:0\" \n",
    "else:  \n",
    "    device = \"cpu\"\n",
    "print(f'Your device is \"{device}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form a list of directories which contain train or val samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_data_paths(path: str):\n",
    "    \"\"\"\n",
    "    input: path to train or val dir with 'clean' and 'noise' dirs\n",
    "    output: list of paths to clean data\n",
    "    \"\"\"\n",
    "    clean = 'clean/'\n",
    "    list_clean = []\n",
    "    with os.scandir(path+clean) as entries:\n",
    "        for entry in entries:\n",
    "            if entry.is_dir():\n",
    "                list_clean.append(entry.path)\n",
    "    return sorted(list_clean)\n",
    "\n",
    "def check_data_paths(paths_list, show_cnt = 10):\n",
    "    \"\"\"\n",
    "    outputs number of input list elements and their values\n",
    "    \"\"\"\n",
    "    print(f'input list length = {len(paths_list)}')\n",
    "    for i, elem in enumerate(paths_list):\n",
    "        print(elem)\n",
    "        if i==show_cnt:\n",
    "            break\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, transpose and check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_clean_data_paths(clean_data_paths: list):\n",
    "    \"\"\"\n",
    "    Loading numpy arrays situated on input list paths.\n",
    "    input: paths to clean data\n",
    "    output: loaded clean and noisy data to lists of np.ndarrays    \n",
    "    \"\"\"\n",
    "    clean_data_list = []\n",
    "    noisy_data_list = []\n",
    "\n",
    "    #scans every dir path\n",
    "    for path in clean_data_paths:\n",
    "        #scans every file in current dir\n",
    "        with os.scandir(path) as entries:\n",
    "            for entry in entries:\n",
    "                if entry.is_file():\n",
    "                    clean_path = entry.path\n",
    "                    noisy_path = entry.path.replace('clean', 'noisy',1)\n",
    "\n",
    "                    clean_data_list.append(np.load(clean_path).T)\n",
    "                    noisy_data_list.append(np.load(noisy_path).T)\n",
    "    return clean_data_list, noisy_data_list\n",
    "\n",
    "def check_loaded_data(data1_list, data2_list, check_part=0.1, check_shapes=True, print_shapes=False, check_data=False):\n",
    "    \"\"\"\n",
    "    Help checking loaded data: if data from 2 lists has same shape and doesn't contain nans, zeros only and so on\n",
    "    \"\"\"\n",
    "    \n",
    "    cnt2check = int(max(1, len(data1_list)*check_part))\n",
    "    samples2check = np.random.choice(range(len(data1_list)), cnt2check, replace=False)\n",
    "    \n",
    "    different_shape = False\n",
    "    for index in samples2check:\n",
    "        if data1_list[index].shape != data2_list[index].shape:\n",
    "            different_shape = True\n",
    "            print(f'Difference in shape is detected!'\n",
    "                 f'\\t{data1_list[index].shape} vs {data2_list[index].shape}')\n",
    "            break\n",
    "        if print_shapes:\n",
    "            print(data1_list[index].shape)\n",
    "    \n",
    "    if check_data:\n",
    "        for index in samples2check:\n",
    "            print(f'mean1 = {np.mean(data1_list[index]):5.3}, std1 = {np.std(data1_list[index]):5.3}\\t'\n",
    "                  f'mean2 = {np.mean(data2_list[index]):5.3}, std2 = {np.std(data2_list[index]):5.3}')\n",
    "        \n",
    "def get_data_statistics(data):\n",
    "    \"\"\"\n",
    "    Prints some data statistics and plots histogram of data lengths\n",
    "    \"\"\"\n",
    "    dataLens = []\n",
    "    for d in data:\n",
    "        dataLens.append(d.shape[1])\n",
    "    dataLens = np.array(dataLens)\n",
    "    maxLen = np.max(dataLens)\n",
    "    minLen = np.min(dataLens)\n",
    "    meanLen = np.mean(dataLens)\n",
    "    stdLen = np.std(dataLens)\n",
    "    print(f'Data length params: min={minLen}, max={maxLen}, mean={meanLen}, std={stdLen}')\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.histplot(data=dataLens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use PyTorch functionality for batch construction:\n",
    "  - custom Dataset class to store data with different sizes,\n",
    "  - customized collate_fn to form batches of different shape elements in DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionDS(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class.\n",
    "    Maintains\n",
    "        data - clean and noisy data\n",
    "        targets - noise residuals of the same size as noisy or clean data\n",
    "    \"\"\"\n",
    "    def __init__(self, clean, noisy):\n",
    "        #initialize class object\n",
    "        \n",
    "        #concatenate clean and noisy data lists\n",
    "        self.data = noisy\n",
    "        #generate corresponding targets\n",
    "        self.targets = [noisy[i]-clean[i] for i in range(len(clean))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        #standard interface function for Dataset\n",
    "        if self.data != None:\n",
    "            return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        #standard interface function for Dataset\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        sample = [self.data[idx], self.targets[idx]] #{'mel_data': self.data[idx], 'target': self.targets[idx]}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_function(batch):\n",
    "    \"\"\"\n",
    "    A customization of default PyTorch collate_fn function\n",
    "    Forms batch from items of different size:\n",
    "        looks for min item length and then randomly \n",
    "        cuts parts of min length from each item\n",
    "    \"\"\"\n",
    "    #extract data from input batch\n",
    "    data = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    min_len = 100000\n",
    "    for elem in data:\n",
    "        min_len = min(min_len, elem.shape[1])\n",
    "    \n",
    "    #slicing data and targets to size of minimum element\n",
    "    temp_pairs = [random_cut(elem[0], elem[1], min_len) for elem in zip(data, targets)]\n",
    "    data = [elem[0] for elem in temp_pairs]\n",
    "    targets = [elem[1] for elem in temp_pairs]\n",
    "    return [torch.tensor(data).float(), torch.tensor(targets).float()]\n",
    "        \n",
    "    \n",
    "def random_cut(array1, array2, slice_len):\n",
    "    \"\"\"\n",
    "    Help function to randomly cut simultaneously 2 arrays through slicing\n",
    "    \"\"\"\n",
    "    start_cut_range = array1.shape[1] - slice_len\n",
    "    random_start = np.random.randint(0, start_cut_range+1)\n",
    "\n",
    "    return (array1[:, random_start: random_start+slice_len], array2[:, random_start: random_start+slice_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DataLoader(clean_data_list, noisy_data_list, batch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    Returns dataloaders for further training and evaluating\n",
    "    Input:\n",
    "        clean_data_list - list of numpy arrays with clean data\n",
    "        noisy_data_list - list of numpy arrays with noisy data\n",
    "        batch_size - defines batch size\n",
    "    \"\"\"\n",
    "    dataset = DetectionDS(clean_data_list, noisy_data_list)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, \n",
    "                            collate_fn=collate_function, pin_memory=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition\n",
    "For noise reduction problem we will use 15-layer DnCNN-like CNN with BatchNorm layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseReductionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Model definition.\n",
    "    It is fully-convolutional network, where objective is noise residual part.\n",
    "    If Y is residual, X is clean data and Y' is noisy data, then CNN output will be \n",
    "        Y = Y'- X,\n",
    "    and accordingly our objective will be computed as \n",
    "        X = Y' - Y\n",
    "    Layers summary:\n",
    "        There are 20 layers:\n",
    "            1 Conv1D layer->ReLU,\n",
    "            18 Conv1D layers->BatchNorm->RelU,\n",
    "            1 Conv1D layer\n",
    "    \"\"\"\n",
    "    def __init__(self, nLayers=20):\n",
    "        super(NoiseReductionModel, self).__init__()\n",
    "        #DnCNN like architecture\n",
    "        self.repeated_layers = nLayers-2\n",
    "        self.conv_list = nn.ModuleList()\n",
    "        for nLayer in range(self.repeated_layers):\n",
    "            self.conv_list.append(nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1))\n",
    "            self.conv_list.append(nn.BatchNorm1d(64))\n",
    "            self.conv_list.append(nn.ReLU())\n",
    "            if nLayer>0 and nLayer%4==0:\n",
    "                self.conv_list.append(nn.Dropout(0.5))\n",
    "        \n",
    "        self.ConvFirst = nn.Conv1d(in_channels=80, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.ConvLast = nn.Conv1d(in_channels=64, out_channels=80, kernel_size=3, padding=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = F.relu(self.ConvFirst(inputs))\n",
    "        \n",
    "        for layer in self.conv_list:\n",
    "            x = layer(x)\n",
    "        \n",
    "        x = self.ConvLast(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions for training epoch and evaluating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Computes output for data in dataloader and returns loss.\n",
    "    Input: \n",
    "        model - trained model, \n",
    "        dataloader - DataLoader with evaluating data,\n",
    "        device - chosen device\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    running_loss = 0.\n",
    "    handled_samples = 0\n",
    "    for batch_number, data in enumerate(dataloader):\n",
    "        samples, targets = data[0], data[1]\n",
    "        samples, targets = samples.to(device) , targets.to(device)\n",
    "\n",
    "        predictions = model(samples)\n",
    "        targets = targets\n",
    "        samples = samples\n",
    "        mel_predictions = samples - predictions\n",
    "        mel_targets = samples - targets\n",
    "        \n",
    "        running_loss += criterion(mel_predictions, mel_targets).item()\n",
    "        handled_samples += targets.shape[0]\n",
    "    \n",
    "    return running_loss/handled_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising evaluation\n",
    "Loading best trained model and compute its MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loaded_denoising_model = NoiseReductionModel(15)\n",
    "model_state_dict = torch.load(f'denoiser_epoch4.pth')\n",
    "loaded_denoising_model.load_state_dict(model_state_dict)\n",
    "loaded_denoising_model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean_paths_list = get_clean_data_paths(test_path)\n",
    "test_clean_data, test_noisy_data = get_data_from_clean_data_paths(test_clean_paths_list[:])\n",
    "test_dataloader = get_DataLoader(test_clean_data, test_noisy_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.05488\n"
     ]
    }
   ],
   "source": [
    "test_loss = Evaluate(loaded_denoising_model, test_dataloader, device)\n",
    "print(f'MSE = {test_loss:6.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results visualisation\n",
    "For each element with the index in list2visualize calculate MSE loss and visualize noisy->predicted->clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateAndVisualize(model, clean_data, noisy_data, device, list_of_indeces_to_check):\n",
    "    \"\"\"\n",
    "    Computes output for data of certain indeces, returns loss and draw heatmaps of data.\n",
    "    Input: \n",
    "        model - trained model,\n",
    "        clean_data - clean data,\n",
    "        noisy_data - noisy data,\n",
    "        device - chosen device,\n",
    "        list_of_indeces_to_check - list of indeces of data to compute loss and draw figures\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    dataset = DetectionDS(clean_data, noisy_data)\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    for data_idx in list_of_indeces_to_check:\n",
    "        data = dataset[data_idx]\n",
    "        sample, target = torch.tensor(data[0]).unsqueeze(0).float(), torch.tensor(data[1]).unsqueeze(0).float()\n",
    "        sample, target = sample.to(device) , target.to(device)\n",
    "\n",
    "        prediction = model(sample)\n",
    "        mel_prediction = sample - prediction\n",
    "        mel_target = sample - target\n",
    "        \n",
    "        loss = criterion(mel_prediction, mel_target).item()\n",
    "        \n",
    "        sample = sample.squeeze(0).cpu().detach().numpy()\n",
    "        mel_prediction = mel_prediction.squeeze(0).cpu().detach().numpy()\n",
    "        mel_target = mel_target.squeeze(0).cpu().detach().numpy()\n",
    "        print(f'mel-spectrogram idx={data_idx}, loss={loss:5.3}\\n')\n",
    "        \n",
    "        plt.figure(figsize=(15,15))\n",
    "        plt.subplot(311)\n",
    "        plt.title(f'Noisy sample_{data_idx}')\n",
    "        sns.heatmap(sample, cmap=\"YlGnBu\")\n",
    "        plt.subplot(312)\n",
    "        plt.title(f'Predicted sample_{data_idx}')\n",
    "        sns.heatmap(mel_prediction, cmap=\"YlGnBu\")\n",
    "        plt.subplot(313)\n",
    "        plt.title(f'Clean sample_{data_idx}')\n",
    "        sns.heatmap(mel_target, cmap=\"YlGnBu\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if len(list2visualize) > 0:\n",
    "    EvaluateAndVisualize(loaded_denoising_model, test_clean_data, test_noisy_data, device, list2visualize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
